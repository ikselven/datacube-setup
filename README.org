* Datacube Installation Scripts [WIP]

Scripts to aid in setup of the Datacube software and its web interface Datacube
UI.

Please read the whole README at least once prior to using any of the scripts.

** Foreword
This is a *work in progress* and is still incomplete. All scripts are written in
batch style, automatically aborting on error thanks to bash's =set -e=. There 
is manual intervention required, e.g. to enter the password for sudo,
enter some information this installer needs or to deal with the configuration 
dialogs of the Debian package manager. At the moment, the scripts are not
guaranteed to be idempotent, so running the scripts twice on the same system
might fail.

The scripts are developed on a Debian Stretch system, and have been additionally
tested on an Ubuntu 16.04 using upstart instead of systemd. Requirements such as
PostgreSQL, Postfix and Apache are installed via =apt= while Python packages
are installed via =conda=, falling back on =pip= if necessary.

On Ubuntu releases other than 16.04, the scripts may work. Be aware though,
that as package names, package versions or paths might be different, manual
intervention might be necessary to make the scripts run cleanly.

** Warning
Originally, the scripts have been developed with an empty environment in mind,
i.e. no PostgreSQL, Apache Webserver or Postfix pre-installed. The setup of
PostgreSQL has already been adjusted to give the user the option to configure
it themselves. Apache and Postfix setup currently do *not* offer this option yet.

** Preparation
- setup a (virtual) machine with Debian Stretch
- create a regular user with sudo permission
- install [[https://www.anaconda.com/download/#linux][Anaconda]] in your home
  directory
- create a directory called =datacube= in your home directory
- download satellite data that should be added to the datacube

** Setup of the Datacube
Change into =~/datacube= and clone this repository. Run
=datacube-setup/scripts/01-install-datacube.sh= to install the Datacube
software, PostgreSQL and initialize the database for the Datacube. If you
want to configure PostgreSQL manually, this is what the script will change in
the default configuration:

: File: /etc/postgresql/*/main/pg_hba.conf
: ------------------------------------------------------------------------------
: <file content omitted>
: host    all             ${DB_USER}       samenet                 trust

This line is added at the end, =${DB_USER}= refers to the username the
scripts asks for.

: File: /etc/postgresql/*/main/postgresql.conf
: ------------------------------------------------------------------------------
: max_connections = 1000
: unix_socket_directories = '/var/run/postgresql,/tmp'
: shared_buffers = 4096MB
: work_mem = 64MB
: maintenance_work_mem = 256MB
: timezone = 'UTC'

For more info, see [[https://github.com/ceos-seo/data_cube_ui/blob/master/docs/datacube_install.md#postgresql-configuration][https://github.com/ceos-seo/data_cube_ui/blob/master/docs/datacube_install.md#postgresql-configuration]].

Run =datacube-setup/scripts/02-configure-data-import-environment.sh= to
install python packages required for data import and create the required
directory structure.

** Setup of the Datacube UI

Run =datacube-setup/scripts/03-install-datacube-ui.sh=. This will install the
Datacube UI itself and required software such as: Apache 2 Webserver, Redis,
ImageMagick, Django and Postfix.

After that, follow the instructions at the end of the script:
- complete the configuration and setup of Django
- apply workarounds of the 'Known Issues' section, if necessary

Finally, to setup Celery as a daemon, run =datacube-setup/scripts/04-configure-celery.sh=.

** Import/Ingest Data
Unpack the downloaded scenes into =~/datacube/data/original_data=.

Before the ingestion, 3 files need to be created:
- a product type definition (YAML),
- a prepare script (Python) and
- an ingestion configuration (YAML).

Examples for these files can be found in this repository in =metadata=, in the
[[https://github.com/ceos-seo/agdc-v2/tree/master/ingest][AGDC repository]],
in [[https://github.com/GRIDgva/SwissDataCube/blob/master/docs/customization/create_and_ingest_new_product.md][this guide for the Swiss Datacube]] and in the 
[[https://github.com/opendatacube/datacube-core/tree/develop/docs/config_samples][Datacube repository]].
For documentation about those files, see the [[https://datacube-core.readthedocs.io/en/latest/][Datacube documentation]].

Adjust all file paths in =datacube-setup/scripts/05-ingest-data.sh= to correctly refer to the 
original data and the 3 files from above. Run the script.

** Known Issues
*** =ImportError: /lib/x86_64-linux-gnu/libz.so.1: version `ZLIB_1.2.9' not found= in Apache error log
In the current setup with a conda environment, it may occur, that python code
run by Apache via mod_wsgi has trouble to see some of the libraries installed
in the conda environment. There 2 workarounds for this, both are /hacky/:
1. Download Zlib 1.2.9 and run =./configure prefix=/usr/local/; make; sudo make install=
2. Extend =$PATH= and =$LD_LIBRARY_PATH= in =/etc/apache2/envvars= to include
   the =bin= and the =lib= directory of the conda environment for the datacube.

*** Error "populate() isn't reentrant" in Apache error log
Run "manage.py check" inside the =data_cube_ui= directory and fix the issues
reported there. This message might appear e.g. when using the Datacube UI in
combination with Django 2 without having migrated to Django 2.

** Funding
This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 642088. It is related to the project [[http://swos-service.eu][Satellite-based Wetland Observation Service]] (SWOS) and related work of the [[http://www.eo.uni-jena.de][Friedrich Schiller University Jena - Department for Earth Observation]]. 
